{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col,trim, to_timestamp, date_format\n",
    "from pyspark.sql.window import Window\n",
    "#from pyspark.sql import functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# To build the sparkSession\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"PySpark Assignment\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data file\n",
    "file_path = \"/home/raghava/Projects/Bigdata_class/Input_data/web_visits_data.txt\"\n",
    "\n",
    "# Reading the data\n",
    "sales_data = spark.read.option(\"inferschema\", True).csv(file_path, header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "for col_name in sales_data.columns:\n",
    "    sales_data = sales_data.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "# add timestamp datatype to timestamp column \n",
    "stamped_sales_data = sales_data.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Change the time stamp format\n",
    "time_stamped_data = stamped_sales_data.withColumn(\"Sys_date\", date_format(col(\"timestamp\"), \"MM/dd/yyyy HH:mm:ss\"))    \n",
    "\n",
    "# Specify the HDFS path\n",
    "hdfs_path = \"hdfs://127.0.0.1:9000/raghava/data/enriched\"\n",
    "\n",
    "# Write the DataFrame to HDFS in Parquet format\n",
    "stamped_sales_data.write.parquet(hdfs_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the HDFS path\n",
    "hdfs_path = \"hdfs://127.0.0.1:9000/raghava/data/enriched/part-00000-b428cee2-cdcd-4c60-a993-8734878e39ad-c000.snappy.parquet\"\n",
    "\n",
    "hdfs_parq = spark.read.option(\"inferschema\", True).parquet(hdfs_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-The total number of visits for each title:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+----------+\n",
      "|title                                                 |num_visits|\n",
      "+------------------------------------------------------+----------+\n",
      "|Deal of the Day: Electronics Deals - Best Buy         |994       |\n",
      "|Clearance Electronics Outlet - Best Buy               |982       |\n",
      "|Member Deals - Best Buy                               |969       |\n",
      "|Best Buy | Official Online Store | Shop Now &amp; Save|1007      |\n",
      "|Cart - Best Buy                                       |1018      |\n",
      "|Schedule a Service - Best Buy                         |975       |\n",
      "|Best Buy Top Deals                                    |983       |\n",
      "|Best Buy Support & Customer Service                   |994       |\n",
      "|Track Your Repair - Best Buy                          |1042      |\n",
      "|Remote Support: Geek Squad - Best Buy                 |1036      |\n",
      "+------------------------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating temp table view for querying\n",
    "hdfs_parq.createOrReplaceTempView(\"hdfs_parq_view\")\n",
    "\n",
    "# Querying the data\n",
    "visit_counts = spark.sql(\"SELECT title, COUNT(visitId) AS num_visits FROM hdfs_parq_view GROUP BY title\")\n",
    "\n",
    "# Number of visits for each title\n",
    "visit_counts.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-The Hour of the Day with most visits overall:-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|hour_of_day|num_visits|\n",
      "+-----------+----------+\n",
      "|          4|       474|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "most_visit_hour = spark.sql(\"\"\"\n",
    "    SELECT HOUR(timestamp) AS hour_of_day, COUNT(*) AS num_visits\n",
    "    FROM hdfs_parq_view\n",
    "    GROUP BY HOUR(timestamp)\n",
    "    ORDER BY num_visits DESC\n",
    "    Limit(1)\n",
    "\"\"\")\n",
    "\n",
    "# Most number of visits in the hour of the day\n",
    "most_visit_hour.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Find out the User with most visits:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     name|visit_times|\n",
      "+---------+-----------+\n",
      "|ANVITAD23|        697|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "most_visited_user = spark.sql(\"\"\"\n",
    "    SELECT name, COUNT(name) AS visit_times \n",
    "    FROM hdfs_parq_view \n",
    "    GROUP BY name\n",
    "    ORDER BY visit_times DESC\n",
    "    LIMIT(1)                          \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Most number of visiting user\n",
    "most_visited_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Find out the User with most visits for 'Remote Support: Geek Squad - Best Buy':-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|             name|visit_for_emote|\n",
      "+-----------------+---------------+\n",
      "|MANIDEEPBOYAPATI9|             84|\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "user_for_remote_support = spark.sql(\"\"\"\n",
    "    SELECT name, COUNT(*) as visit_for_emote\n",
    "    FROM hdfs_parq_view \n",
    "    WHERE title = 'Remote Support: Geek Squad - Best Buy'                              \n",
    "    GROUP BY name\n",
    "    ORDER BY visit_for_emote DESC\n",
    "    LIMIT(1) \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Most number of visiting user for 'Remote Support:\n",
    "user_for_remote_support.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-Both 'Best Buy Support & Customer Service' and 'Remote Support: Geek Squad - Best Buy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|name           |visit_for_both|\n",
      "+---------------+--------------+\n",
      "|SRIKANTHPITTA18|153           |\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "user_for_remote_bestbuy_support = spark.sql(\"\"\"\n",
    "    SELECT name, COUNT(*) as visit_for_both\n",
    "    FROM hdfs_parq_view \n",
    "    WHERE title IN ('Best Buy Support & Customer Service','Remote Support: Geek Squad - Best Buy')\n",
    "    GROUP BY name\n",
    "    ORDER BY visit_for_both DESC\n",
    "    LIMIT(1)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Most number of visiting user for 'Remote Support:\n",
    "user_for_remote_bestbuy_support.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-Both 'Best Buy Support & Customer Service' and 'Schedule a Service - Best Buy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|name           |visit_for_both|\n",
      "+---------------+--------------+\n",
      "|SRIKANTHPITTA18|150           |\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "user_for_remote_bestbuy_support = spark.sql(\"\"\"\n",
    "    SELECT name, COUNT(*) as visit_for_both\n",
    "    FROM hdfs_parq_view \n",
    "    WHERE title IN ('Best Buy Support & Customer Service','Schedule a Service - Best Buy')\n",
    "    GROUP BY name\n",
    "    ORDER BY visit_for_both DESC\n",
    "    LIMIT(1)\n",
    "\"\"\")\n",
    "\n",
    "# Most number of visiting user for 'Remote Support:\n",
    "user_for_remote_bestbuy_support.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-The User who has the longest time interval between visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|name    |max_interval_hrs|\n",
      "+--------+----------------+\n",
      "|SASI2025|0.35            |\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "query = \"\"\"\n",
    "WITH visit_diffs AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        timestamp,\n",
    "        LAG(timestamp) OVER (PARTITION BY name ORDER BY timestamp) AS prev_timestamp,\n",
    "        UNIX_TIMESTAMP(timestamp) - UNIX_TIMESTAMP(LAG(timestamp) OVER (PARTITION BY name ORDER BY timestamp)) AS time_diff\n",
    "    FROM hdfs_parq_view\n",
    "    )\n",
    "\n",
    "    SELECT name, ROUND(MAX(time_diff)/3600, 2) AS max_interval_hrs\n",
    "    FROM visit_diffs\n",
    "    GROUP BY name\n",
    "    ORDER BY max_interval_hrs DESC\n",
    "    LIMIT(1)\n",
    "\"\"\"\n",
    "\n",
    "long_interval_user = spark.sql(query)\n",
    "\n",
    "# Most interval time user visiting\n",
    "long_interval_user.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+\n",
      "|name  |min_interval_hrs|\n",
      "+------+----------------+\n",
      "|ABAGAM|0.0             |\n",
      "+------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "query = \"\"\"\n",
    "WITH visit_diffs AS (\n",
    "    SELECT \n",
    "        name,\n",
    "        timestamp,\n",
    "        LAG(timestamp) OVER (PARTITION BY name ORDER BY timestamp) AS prev_timestamp,\n",
    "        UNIX_TIMESTAMP(timestamp) - UNIX_TIMESTAMP(LAG(timestamp) OVER (PARTITION BY name ORDER BY timestamp)) AS time_diff\n",
    "    FROM hdfs_parq_view\n",
    "    )\n",
    "\n",
    "    SELECT name, ROUND(MIN(time_diff)/3600, 2) AS min_interval_hrs\n",
    "    FROM visit_diffs\n",
    "    GROUP BY name\n",
    "    ORDER BY min_interval_hrs\n",
    "    LIMIT(1)\n",
    "\"\"\"\n",
    "\n",
    "less_interval_user = spark.sql(query)\n",
    "\n",
    "# Most interval time user visiting\n",
    "less_interval_user.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
